{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entrenamiento usando las técnicas de HuggingFace"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chequear GPU\n",
        "\n",
        "(Error después porque hay que usar un tipo de modelo distinto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from pynvml import *\n",
        "#\n",
        "#\n",
        "#def print_gpu_utilization():\n",
        "#    nvmlInit()\n",
        "#    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "#    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "#    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "#\n",
        "#\n",
        "#def print_summary(result):\n",
        "#    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "#    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "#    print_gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print_gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import torch\n",
        "#\n",
        "#torch.ones((1, 1)).to(\"cuda\")\n",
        "#\n",
        "#print_gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "#torch.cuda.is_available()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Función para convertir labels en dataset de tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convertir_labels(train_data, val_data):\n",
        "    MIN_VOTOS = 2\n",
        "\n",
        "    if MIN_VOTOS == 3:\n",
        "        # Considerar que un dato pertenece solo si las 3 personas pusieron que pertenece\n",
        "        train_data['Odio'] = np.where(train_data['Odio'] == 3, 1, 0)\n",
        "        val_data['Odio'] = np.where(val_data['Odio'] == 3, 1, 0)\n",
        "        train_data['Mujeres'] = np.where(train_data['Mujeres'] == 3, 1, 0)\n",
        "        val_data['Mujeres'] = np.where(val_data['Mujeres'] == 3, 1, 0)\n",
        "        train_data['Comunidad LGBTQ+'] = np.where(train_data['Comunidad LGBTQ+'] == 3, 1, 0)\n",
        "        val_data['Comunidad LGBTQ+'] = np.where(val_data['Comunidad LGBTQ+'] == 3, 1, 0)\n",
        "        train_data['Comunidades Migrantes'] = np.where(train_data['Comunidades Migrantes'] == 3, 1, 0)\n",
        "        val_data['Comunidades Migrantes'] = np.where(val_data['Comunidades Migrantes'] == 3, 1, 0)\n",
        "        train_data['Pueblos Originarios'] = np.where(train_data['Pueblos Originarios'] == 3, 1, 0)\n",
        "        val_data['Pueblos Originarios'] = np.where(val_data['Pueblos Originarios'] == 3, 1, 0)   \n",
        "    elif MIN_VOTOS == 2:\n",
        "        train_data['Odio'] = np.where((train_data['Odio'] == 3) | (train_data['Odio'] == 2), 1, 0)\n",
        "        val_data['Odio'] = np.where((val_data['Odio'] == 3) | (val_data['Odio'] == 2), 1, 0)\n",
        "        train_data['Mujeres'] = np.where((train_data['Mujeres'] == 3) | (train_data['Mujeres'] == 2), 1, 0)\n",
        "        val_data['Mujeres'] = np.where((val_data['Mujeres'] == 3) | (val_data['Mujeres'] == 2), 1, 0)\n",
        "        train_data['Comunidad LGBTQ+'] = np.where((train_data['Comunidad LGBTQ+'] == 3) | (train_data['Comunidad LGBTQ+'] == 2), 1, 0)\n",
        "        val_data['Comunidad LGBTQ+'] = np.where((val_data['Comunidad LGBTQ+'] == 3) | (val_data['Comunidad LGBTQ+'] == 2), 1, 0)\n",
        "        train_data['Comunidades Migrantes'] = np.where((train_data['Comunidades Migrantes'] == 3) | (train_data['Comunidades Migrantes'] == 2), 1, 0)\n",
        "        val_data['Comunidades Migrantes'] = np.where((val_data['Comunidades Migrantes'] == 3) | (val_data['Comunidades Migrantes'] == 2), 1, 0)\n",
        "        train_data['Pueblos Originarios'] = np.where((train_data['Pueblos Originarios'] == 3) | (train_data['Pueblos Originarios'] == 2), 1, 0)\n",
        "        val_data['Pueblos Originarios'] = np.where((val_data['Pueblos Originarios'] == 3) | (val_data['Pueblos Originarios'] == 2), 1, 0)\n",
        "\n",
        "    return train_data, val_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funcion de limpieza de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import spacy_spanish_lemmatizer\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import spacy_spanish_lemmatizer\n",
        "\n",
        "stemmer = SnowballStemmer(\"spanish\")\n",
        "nlp = spacy.load(\"es_core_news_lg\")\n",
        "#nlp.replace_pipe(\"lemmatizer\", \"spanish_lemmatizer\")\n",
        "stop_words = stopwords.words(\"spanish\")\n",
        "\n",
        "def clean_text(text, len_words):\n",
        "    TEXT_CLEANING_RE = \"@\\S+\" # separado porque solo agarraba la primera etiqueta\n",
        "    TEXT_CLEANING_RE2 = \"https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "    TEXT_CLEANING_RE_EXTRA = \"[^\\w\\s]\"\n",
        "    if len_words == 1:\n",
        "        text = re.sub(r'\\b\\w{1}\\b', '', str(text).lower()).strip()\n",
        "    elif len_words == 2:\n",
        "        text = re.sub(r'\\b\\w{1,2}\\b', '', str(text).lower()).strip()\n",
        "    elif len_words == 3:\n",
        "        text = re.sub(r'\\b\\w{1,3}\\b', '', str(text).lower()).strip()\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    text = re.sub(TEXT_CLEANING_RE2, ' ', str(text).lower()).strip()\n",
        "    text = re.sub(TEXT_CLEANING_RE_EXTRA, ' ', str(text).lower()).strip()\n",
        "    return text\n",
        "\n",
        "def preprocess(text,cleaning=True, stopwords=True, stemming=False, lemmatizer=False, len_words=1):\n",
        "    text = unidecode.unidecode(text)\n",
        "    if cleaning:\n",
        "        text = clean_text(text, len_words)\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if (not stopwords) or (stopwords and (token not in stop_words)):\n",
        "            if stemming:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            elif lemmatizer:\n",
        "                doc = nlp(token)\n",
        "                tokens.append(unidecode.unidecode(doc[0].lemma_))\n",
        "            else:\n",
        "                tokens.append(token)    \n",
        "    text2 = \" \".join(tokens)\n",
        "    if cleaning:\n",
        "        text2 = clean_text(text2, len_words)\n",
        "    return text2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cargar datasets\n",
        "\n",
        "Notar que estos son de tipo \"Dataset\", un tipo que se usa en HuggingFace comunmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'Odio', 'Mujeres', 'Comunidad LGBTQ+', 'Comunidades Migrantes', 'Pueblos Originarios'],\n",
            "    num_rows: 1917\n",
            "})\n",
            "{'text': 'choro', 'Odio': 0, 'Mujeres': 0, 'Comunidad LGBTQ+': 0, 'Comunidades Migrantes': 0, 'Pueblos Originarios': 0}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "\n",
        "csv = pd.read_csv(\"data/tweets_train.csv\")\n",
        "\n",
        "TRAIN_SIZE = 0.85\n",
        "\n",
        "train_data = csv.sample(frac=1).reset_index(drop=True)\n",
        "train_data, val_data = train_data.iloc[:int(TRAIN_SIZE*len(train_data)), :], train_data.iloc[int(TRAIN_SIZE*len(train_data)):, :]\n",
        "train_data, val_data = convertir_labels(train_data, val_data)\n",
        "train_data.drop(columns=['Unnamed: 0', 'tweet_id', 'author_id', 'conversation_id'], inplace=True)\n",
        "val_data.drop(columns=['Unnamed: 0', 'tweet_id', 'author_id', 'conversation_id'], inplace=True)\n",
        "\n",
        "train_data['text'] = train_data['text'].apply(lambda x: preprocess(x, cleaning=True, stopwords=False, stemming=False, lemmatizer=False, len_words=0))\n",
        "val_data['text'] = val_data['text'].apply(lambda x: preprocess(x, cleaning=True, stopwords=False, stemming=False, lemmatizer=False, len_words=0))\n",
        "\n",
        "# convertir a dataset de huggingface\n",
        "train_dataset = Dataset.from_pandas(train_data)\n",
        "val_dataset = Dataset.from_pandas(val_data)\n",
        "\n",
        "print(train_dataset)\n",
        "print(train_dataset[1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Convertir columnas en labels para losd datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Odio',\n",
              " 'Mujeres',\n",
              " 'Comunidad LGBTQ+',\n",
              " 'Comunidades Migrantes',\n",
              " 'Pueblos Originarios']"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = ['Odio', 'Mujeres', 'Comunidad LGBTQ+', 'Comunidades Migrantes', 'Pueblos Originarios']\n",
        "id2label = {idx: label for idx, label in enumerate(labels)}\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "labels"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                  \r"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_dataset(data):\n",
        "    # Keys of the returned dictionary will be added to the dataset as columns\n",
        "    return tokenizer(data[\"sentence\"], truncation=True)\n",
        "\n",
        "def preprocess_data(examples):\n",
        "    text = examples['text']\n",
        "    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=128)\n",
        "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
        "    labels_matrix = np.zeros((len(text), len(labels)))\n",
        "    for idx, label in enumerate(labels):\n",
        "        labels_matrix[:, idx] = labels_batch[label]\n",
        "    encoding['labels'] = labels_matrix\n",
        "    return encoding\n",
        "\n",
        "encoded_train = train_dataset.map(preprocess_data, batched=True, remove_columns=labels)\n",
        "encoded_val = val_dataset.map(preprocess_data, batched=True, remove_columns=labels)\n",
        "\n",
        "\n",
        "\n",
        "encoded_train.set_format(\"torch\")\n",
        "encoded_val.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 'dona bachelet nos dejo el pais infectado con tanto haitiano son sucios', 'input_ids': tensor([  101,  1274,  1161,   171, 12804,  5765,  1185,  1116,  1260,  5077,\n",
            "         8468,   185, 15837,  1107, 11916,  9359, 14255, 15925,  2430,  5871,\n",
            "        17030,  7428,  1488, 28117,  8174,  1116,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([1., 0., 0., 1., 0.])}\n",
            "dict_keys(['text', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "print(encoded_train[2])\n",
        "print(encoded_train[2].keys())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparar dataset y modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [00:07<00:00, 61.6MB/s] \n",
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', num_labels=5, \n",
        "                                                             problem_type=\"multi_label_classification\", id2label=id2label, \n",
        "                                                             label2id=label2id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "metric_name = \"f1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"bert-finetuned-sem_eval-english\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    #push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "    \n",
        "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
        "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
        "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "    # finally, compute metrics\n",
        "    y_true = labels\n",
        "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
        "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    # return as dictionary\n",
        "    metrics = {'f1': f1_micro_average,\n",
        "               'roc_auc': roc_auc,\n",
        "               'accuracy': accuracy}\n",
        "    return metrics\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, \n",
        "            tuple) else p.predictions\n",
        "    result = multi_label_metrics(\n",
        "        predictions=preds, \n",
        "        labels=p.label_ids)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_train[0]['labels'].type()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  101,   175,  8558, 21359, 23015,  1279,   178,  1197,   170,  2495,\n",
              "         1508,  1161,   102,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_train['input_ids'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=tensor(0.7357, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[ 0.0955, -0.0559,  0.2567, -0.1156,  0.2113]],\n",
              "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(input_ids=encoded_train['input_ids'][0].unsqueeze(0), labels=encoded_train[0]['labels'].unsqueeze(0))\n",
        "outputs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_train,\n",
        "    eval_dataset=encoded_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predecir"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EkFGgLo7s8tE",
        "bsVED_Pw1WJO",
        "XzdMTP1z_Xhe"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
